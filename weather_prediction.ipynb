This notebook will be used to see which supervised learning model will be best for predicting the weather using information like precipitation, max temperature, min temperature, and wind. The models tested will be logisitic regression, decision tree, K nearest neighbor (KNN), random forest, and support vector classification. For comparing model performance, accruacy, recall, precision, and f1-score will be calculated. 
```
%matplotlib inline
import random
import numpy as np
import scipy as sp
import scipy.stats as stats
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
# Set color map to have light blue background
sns.set()
import sklearn
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import roc_auc_score, roc_curve, confusion_matrix, accuracy_score, balanced_accuracy_score, recall_score, precision_score, f1_score
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.svm import SVC
```

```
# Read in the data.
df = pd.read_csv("/kaggle/input/weather-prediction/seattle-weather.csv")
```

# Data cleaning
```
# Print the head of the data and the data types.
print(df.head())
print(df.info())
```

```
# For a slightly cleaner dataset, remove the data column.
df = df.drop(['date'], axis=1)
print(df.head())
```

```
# Check for any NAs.
print("Any NAs:" , df.isnull().values.any())
```

```
# See if all values make sense.
print(df.describe())
```

There were no NAs in the data. The minimum values for each column seem reasonable. The maximum precipitation seems a bit high so I would like to investigate that before moving on. 

```
# Check the top 10 precipitation values. 
df.nlargest(10, ['precipitation'])
```

The data appears to have no outliers. To confirm I will make boxplots. 

```
df.boxplot(column=['precipitation', 'temp_max', 'temp_min', 'wind'])
```

The precipitation column definitely has some values which are really high but it isn't just one value but a group. For this reason, I will keep these values in the dataset.

```
# Check correlations.
sns.set(font_scale=1)
pp = sns.pairplot(df, diag_kind = 'kde',\
             x_vars=['precipitation', 'temp_max', 'temp_min', 'wind'], \
             y_vars=['precipitation', 'temp_max', 'temp_min', 'wind'])
pp.set(yticklabels=[]) 
pp.set(xticklabels=[])
```

```
features = ['precipitation', 'temp_max', 'temp_min', 'wind']

# Create heatmap of correlations. 
plt.figure(figsize=(16, 8))
hp = sns.heatmap(data = df[features].corr(), vmin = -1, vmax = 1, annot=True, cmap='coolwarm', fmt='.2f')
hp.set_title('Correlation Heatmap for Weather data', fontdict={'fontsize':12}, pad=12)
```

Minimum and maximum temperatures are correlated but that makes sense because there are usually similar ranges between min and max temperatures throughout the day/year. 

```
# Check counts of the column we are trying to predict (weather).
df['weather'].value_counts()
```
Based on the counts, we can expect to predict more rain, sun, and fog. We can also expect to incorrectly predict drizzle and snow for rain, sun, and fog. This is because when we are training, there will be less cases for drizzle and snow and therefore the model will not "learn" a good pattern for when the weather is drizzle or snow. 

# Analysis

```
random.seed(12345)

# Set the data into x and y. 
features = ['precipitation', 'temp_max', 'temp_min', 'wind']
x = df[features]
y = df['weather']

# Split the data into 80% training and 20% testing datasets.  
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=.20, random_state=12345)

# View dimensions.
print("x_train:", x_train.shape)
print("x_test:",x_test.shape)
print("y_train:",y_train.shape)
print("y_test:",y_test.shape)
```

## Logisitic Regression

```
# Create a parameter grid. 
param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100]}

# Create model. 
lin_model = LogisticRegression(random_state=12345, solver='liblinear')

# Run all options in the parameter grid and fit the data to the best one. 
grid = GridSearchCV(lin_model, param_grid, cv=3).fit(x_train, y_train)
print(f"Best parameters: {grid.best_params_}")
print(f"Best cross-validation accuracy score: {grid.best_score_}")

# Create a confusion matrix. 
conf_mat = confusion_matrix(y_test, grid.predict(x_test))

# Create a heatmap.
plt.figure(figsize=(10,7))
sns.heatmap(conf_mat, annot=True, cmap='Blues', fmt='g')

# Add labels to the plot.
class_names = np.unique(y_test)
plt.xticks(ticks=np.arange(len(class_names)), labels=class_names)
plt.yticks(ticks=np.arange(len(class_names)), labels=class_names)
plt.xlabel('Predicted')
plt.ylabel('True')
plt.xticks(ticks=[0.5, 1.5, 2.5, 3.5, 4.5], labels=class_names)
plt.yticks(ticks=[0.5, 1.5, 2.5, 3.5, 4.5], labels=class_names)
plt.title('Confusion Matrix')

plt.show()
```

```
# Get the accuracy, recall, precision, and f1-score to compare models. 
print("Accuracy: ", accuracy_score(y_test, grid.predict(x_test)))
print("Recall: ",recall_score(y_test, grid.predict(x_test), average=None))
print("Recall weighted average: ",recall_score(y_test, grid.predict(x_test), average='weighted'))
print("Precision: ",precision_score(y_test, grid.predict(x_test), average=None))
print("Precision weighted average: ",precision_score(y_test, grid.predict(x_test), average='weighted'))
print("F1-score: ",f1_score(y_test, grid.predict(x_test), average=None))
print("F1-score weighted average: ",f1_score(y_test, grid.predict(x_test), average='weighted'))
```

## Decision Tree

```
# Create a parameter grid. 
param_grid = {'max_depth': [2, 4, 6, 8, 10], 'min_samples_leaf' : [1 ,2, 4, 6, 8, 10]}
# Create model. 
tree_model = DecisionTreeClassifier(random_state=12345)

# Run all options in the parameter grid and fit the data to the best one. 
grid = GridSearchCV(tree_model, param_grid, cv=3).fit(x_train, y_train)
print(f"Best parameters: {grid.best_params_}")
print(f"Best cross-validation accuracy score: {grid.best_score_}")

# Create a confusion matrix. 
conf_mat = confusion_matrix(y_test, grid.predict(x_test))

# Create a heatmap.
plt.figure(figsize=(10,7))
sns.heatmap(conf_mat, annot=True, cmap='Blues', fmt='g')

# Add labels to the plot.
class_names = np.unique(y_test)
plt.xticks(ticks=np.arange(len(class_names)), labels=class_names)
plt.yticks(ticks=np.arange(len(class_names)), labels=class_names)
plt.xlabel('Predicted')
plt.ylabel('True')
plt.xticks(ticks=[0.5, 1.5, 2.5, 3.5, 4.5], labels=class_names)
plt.yticks(ticks=[0.5, 1.5, 2.5, 3.5, 4.5], labels=class_names)
plt.title('Confusion Matrix')

plt.show()
```

```
# Get the accuracy, recall, precision, and f1-score to compare models. 
print("Accuracy: ", accuracy_score(y_test, grid.predict(x_test)))
print("Recall: ",recall_score(y_test, grid.predict(x_test), average=None))
print("Recall weighted average: ",recall_score(y_test, grid.predict(x_test), average='weighted'))
print("Precision: ",precision_score(y_test, grid.predict(x_test), average=None))
print("Precision weighted average: ",precision_score(y_test, grid.predict(x_test), average='weighted'))
print("F1-score: ",f1_score(y_test, grid.predict(x_test), average=None))
print("F1-score weighted average: ",f1_score(y_test, grid.predict(x_test), average='weighted'))
```

## K Nearest Neighbors (KNN)

```
# Create a parameter grid.
param_grid = {'n_neighbors': [3, 5, 7, 9]}

# Create model. 
knn_model = KNeighborsClassifier(n_neighbors=5)

# Run all options in the parameter grid and fit the data to the best one. 
grid = GridSearchCV(knn_model, param_grid, cv=3).fit(x_train, y_train)
print(f"Best parameters: {grid.best_params_}")
print(f"Best cross-validation accuracy score: {grid.best_score_}")

# Create a confusion matrix. 
conf_mat = confusion_matrix(y_test, grid.predict(x_test))

# Create a heatmap.
plt.figure(figsize=(10,7))
sns.heatmap(conf_mat, annot=True, cmap='Blues', fmt='g')

# Add labels to the plot.
class_names = np.unique(y_test)
plt.xticks(ticks=np.arange(len(class_names)), labels=class_names)
plt.yticks(ticks=np.arange(len(class_names)), labels=class_names)
plt.xlabel('Predicted')
plt.ylabel('True')
plt.xticks(ticks=[0.5, 1.5, 2.5, 3.5, 4.5], labels=class_names)
plt.yticks(ticks=[0.5, 1.5, 2.5, 3.5, 4.5], labels=class_names)
plt.title('Confusion Matrix')

plt.show()
```

```
# Get the accuracy, recall, precision, and f1-score to compare models. 
print("Accuracy: ", accuracy_score(y_test, grid.predict(x_test)))
print("Recall: ",recall_score(y_test, grid.predict(x_test), average=None))
print("Recall weighted average: ",recall_score(y_test, grid.predict(x_test), average='weighted'))
print("Precision: ",precision_score(y_test, grid.predict(x_test), average=None))
print("Precision weighted average: ",precision_score(y_test, grid.predict(x_test), average='weighted'))
print("F1-score: ",f1_score(y_test, grid.predict(x_test), average=None))
print("F1-score weighted average: ",f1_score(y_test, grid.predict(x_test), average='weighted'))
```

## Random Forest

```
# Create a parameter grid.  
param_grid = {'n_estimators': [50, 100, 150], 'max_depth': [2, 4, 6, 8, 10]}

# Create model. 
rf_model = RandomForestClassifier(random_state=12345)

# Run all options in the parameter grid and fit the data to the best one. 
grid = GridSearchCV(rf_model, param_grid, cv=3).fit(x_train, y_train)
print(f"Best parameters: {grid.best_params_}")
print(f"Best cross-validation accuracy score: {grid.best_score_}")

# Create a confusion matrix.
conf_mat = confusion_matrix(y_test, grid.predict(x_test))


# Create a heatmap.
plt.figure(figsize=(10,7))
sns.heatmap(conf_mat, annot=True, cmap='Blues', fmt='g')

# Add labels to the plot.
class_names = np.unique(y_test)
plt.xticks(ticks=np.arange(len(class_names)), labels=class_names)
plt.yticks(ticks=np.arange(len(class_names)), labels=class_names)
plt.xlabel('Predicted')
plt.ylabel('True')
plt.xticks(ticks=[0.5, 1.5, 2.5, 3.5, 4.5], labels=class_names)
plt.yticks(ticks=[0.5, 1.5, 2.5, 3.5, 4.5], labels=class_names)
plt.title('Confusion Matrix')

plt.show()
```

```
# Get the accuracy, recall, precision, and f1-score to compare models. 
print("Accuracy: ", accuracy_score(y_test, grid.predict(x_test)))
print("Recall: ",recall_score(y_test, grid.predict(x_test), average=None))
print("Recall weighted average: ",recall_score(y_test, grid.predict(x_test), average='weighted'))
print("Precision: ",precision_score(y_test, grid.predict(x_test), average=None))
print("Precision weighted average: ",precision_score(y_test, grid.predict(x_test), average='weighted'))
print("F1-score: ",f1_score(y_test, grid.predict(x_test), average=None))
print("F1-score weighted average: ",f1_score(y_test, grid.predict(x_test), average='weighted'))
```

## Support Vector

```
# Create a parameter grid. 
param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100], 'gamma': [0.1, 1, 5, 10, 100]}

# Create model. 
svc_model = SVC(random_state=12345)

# Run all options in the parameter grid and fit the data to the best one. 
grid = GridSearchCV(svc_model, param_grid, cv=3).fit(x_train, y_train)
print(f"Best parameters: {grid.best_params_}")
print(f"Best cross-validation accuracy score: {grid.best_score_}")

# Create a confusion matrix. 
conf_mat = confusion_matrix(y_test, grid.predict(x_test))


# Create a heatmap.
plt.figure(figsize=(10,7))
sns.heatmap(conf_mat, annot=True, cmap='Blues', fmt='g')

# Add labels to the plot.
class_names = np.unique(y_test)
plt.xticks(ticks=np.arange(len(class_names)), labels=class_names)
plt.yticks(ticks=np.arange(len(class_names)), labels=class_names)
plt.xlabel('Predicted')
plt.ylabel('True')
plt.xticks(ticks=[0.5, 1.5, 2.5, 3.5, 4.5], labels=class_names)
plt.yticks(ticks=[0.5, 1.5, 2.5, 3.5, 4.5], labels=class_names)
plt.title('Confusion Matrix')

plt.show()
```

```
# Get the accuracy, recall, precision, and f1-score to compare models. 
print("Accuracy: ", accuracy_score(y_test, grid.predict(x_test)))
print("Recall: ",recall_score(y_test, grid.predict(x_test), average=None))
print("Recall weighted average: ",recall_score(y_test, grid.predict(x_test), average='weighted'))
print("Precision: ",precision_score(y_test, grid.predict(x_test), average=None))
print("Precision weighted average: ",precision_score(y_test, grid.predict(x_test), average='weighted'))
print("F1-score: ",f1_score(y_test, grid.predict(x_test), average=None))
print("F1-score weighted average: ",f1_score(y_test, grid.predict(x_test), average='weighted'))
```

| Model               | Accuracy | Recall | Precision | F1-score |
|---------------------|----------|--------|-----------|----------|
| Logistic Regression | 80.55    | 80.55  | 69.41     | 74.32    |
| Decision Tree       | 80.89    | 80.89  | 74.46     | 76.63    |
| KNN                 | 73.72    | 73.72  | 69.51     | 70.58    |
| Random Forest       | 83.28    | 83.28  | 75.21     | 78.15    |
| SVC                 | 75.77    | 75.77  | 70.37     | 72.74    |


Based on the table, the random forest did the best for predicting what the weather would be based on information like precipitation, max temperature, min temperature, and wind. The random forest had the best accuracy, recall, precision, and f1-score (83.28, 83.28, 75.21, 78.15, repectively) out of all of the models tested.

This project however does not come without limitations. First, the data set size is small and certain weather did not occur often (rain: 641, sun: 640, fog: 101, drizzle: 53, snow: 26). This results in the models not being trained well when conditions of drizzle and snow arise in the test data set and often lead to misclassifications. Along with this, weather rain and sun occur much more often than other weather (fog, drizzle, and snow). This also leads to the models misclassifying testing data and reduces model performance metrics. 
